{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f297ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries for 8-bit/4-bit\n",
    "!pip install -q -U bitsandbytes transformers accelerate optimum auto-gptq\n",
    "\n",
    "# Clone and build llama.cpp for GGUF conversion tools\n",
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "!cd llama.cpp && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e693fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import os\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # Example base model\n",
    "\n",
    "# 1. Save 8-bit Model\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    device_map=\"auto\", \n",
    "    load_in_8bit=True\n",
    ")\n",
    "model_8bit.save_pretrained(\"./quantized/model-int8\")\n",
    "\n",
    "# 2. Save 4-bit Model (NF4)\n",
    "bnb_config_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=bnb_config_4bit, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model_4bit.save_pretrained(\"./quantized/model-int4\")\n",
    "\n",
    "print(\"INT8 and INT4 weights saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!python llama.cpp/convert_hf_to_gguf.py ./quantized/model-int4 \\\n",
    "    --outfile ./quantized/model-f16.gguf \\\n",
    "    --outtype f16\n",
    "\n",
    "# 2. Quantize the GGUF file to 4-bit (q4_0)\n",
    "!cd llama.cpp && ./llama-quantize ../quantized/model-f16.gguf ../quantized/model.gguf q4_0\n",
    "\n",
    "print(\"GGUF Conversion Complete: /quantized/model.gguf created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b95cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(path):\n",
    "    size = os.path.getsize(path) / (1024 * 1024) # MB\n",
    "    return f\"{size:.2f} MB\"\n",
    "\n",
    "print(f\"FP16 (approx): ~250 MB\") # Base OPT-125m\n",
    "print(f\"INT8 Size: {get_size('./quantized/model-int8/adapter_model.bin')}\") \n",
    "print(f\"INT4 Size: {get_size('./quantized/model-int4/adapter_model.bin')}\")\n",
    "print(f\"GGUF Size: {get_size('./quantized/model.gguf')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
