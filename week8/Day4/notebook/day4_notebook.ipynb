{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e5543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core libraries\n",
    "!pip install -q -U transformers accelerate bitsandbytes\n",
    "\n",
    "# Clone and build llama.cpp from source\n",
    "!git clone https://github.com/ggml-org/llama.cpp\n",
    "%cd llama.cpp\n",
    "!cmake -B build\n",
    "!cmake --build build --config Release -j 8\n",
    "%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "\n",
    "prompt = \"Explain KV.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "start = time.time()\n",
    "outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True)\n",
    "end = time.time()\n",
    "\n",
    "tokens_generated = outputs.shape[1]\n",
    "print(f\"Latency: {round(end - start, 3)}s\")\n",
    "print(f\"Tokens/sec: {round(tokens_generated / (end - start), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a92bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cmd_fp16(prompt, max_new_tokens=120):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a health-focused language model. Provide clear, factual, and concise medical explanations.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = fp16_model.generate(inputs, max_new_tokens=max_new_tokens, use_cache=True)\n",
    "    end = time.time()\n",
    "\n",
    "    # Decode only the new tokens\n",
    "    generated_ids = outputs[0][inputs.shape[1]:]\n",
    "    text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "    print(f\"Tokens/sec: {len(generated_ids) / (end - start):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef9036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Ensure path to your GGUF file is correct\n",
    "GGUF_PATH = \"/kaggle/input/modelsday4/gguf/phi3-mini-4k-instruct-q4_k_m.gguf\"\n",
    "PROMPT = \"Explain coronavirus disease, including transmission, symptoms, and prevention.\"\n",
    "\n",
    "!./llama.cpp/build/bin/llama-cli \\\n",
    "  -m {GGUF_PATH} \\\n",
    "  -p \"{PROMPT}\" \\\n",
    "  -n 256 \\\n",
    "  -ngl 33  # Offload layers to GPU"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
