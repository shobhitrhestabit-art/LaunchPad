{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T19:27:16.763656Z",
     "iopub.status.busy": "2026-01-29T19:27:16.762951Z",
     "iopub.status.idle": "2026-01-29T19:34:16.462738Z",
     "shell.execute_reply": "2026-01-29T19:34:16.461787Z",
     "shell.execute_reply.started": "2026-01-29T19:27:16.763617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q -U transformers accelerate bitsandbytes\n",
    "\n",
    "# Clone and build llama.cpp from source\n",
    "!git clone https://github.com/ggml-org/llama.cpp\n",
    "%cd llama.cpp\n",
    "!cmake -B build\n",
    "!cmake --build build --config Release -j 8\n",
    "%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T19:34:20.806888Z",
     "iopub.status.busy": "2026-01-29T19:34:20.806583Z",
     "iopub.status.idle": "2026-01-29T19:35:06.141273Z",
     "shell.execute_reply": "2026-01-29T19:35:06.140422Z",
     "shell.execute_reply.started": "2026-01-29T19:34:20.806856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "\n",
    "prompt = \"Explain Coronavirus.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "start = time.time()\n",
    "outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True)\n",
    "end = time.time()\n",
    "\n",
    "tokens_generated = outputs.shape[1]\n",
    "print(f\"Latency: {round(end - start, 3)}s\")\n",
    "print(f\"Tokens/sec: {round(tokens_generated / (end - start), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T20:13:43.948627Z",
     "iopub.status.busy": "2026-01-29T20:13:43.948252Z",
     "iopub.status.idle": "2026-01-29T20:13:54.790867Z",
     "shell.execute_reply": "2026-01-29T20:13:54.790228Z",
     "shell.execute_reply.started": "2026-01-29T20:13:43.948596Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import psutil\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"/kaggle/input/model-int4/int4\" \n",
    "\n",
    "# 1. Define the quantization config properly\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 2. Load model with the config object\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    device_map=\"auto\", \n",
    "    quantization_config=quant_config # Use the config object here\n",
    ")\n",
    "\n",
    "prompt = \"Explain the importance of open-source AI.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# --- Measurement Logic ---\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculations\n",
    "latency = end_time - start_time\n",
    "tokens_gen = outputs.shape[1] - inputs.input_ids.shape[1]\n",
    "vram = torch.cuda.max_memory_allocated() / 1024**3 \n",
    "\n",
    "print(f\"\\n Latency: {latency:.2f}s\")\n",
    "print(f\" Tokens/sec: {tokens_gen / latency:.2f}\")\n",
    "print(f\" Max VRAM Usage: {vram:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T20:15:30.457289Z",
     "iopub.status.busy": "2026-01-29T20:15:30.456222Z",
     "iopub.status.idle": "2026-01-29T20:15:32.386296Z",
     "shell.execute_reply": "2026-01-29T20:15:32.385679Z",
     "shell.execute_reply.started": "2026-01-29T20:15:30.457253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# 1. Setup Path (Based on your file browser screenshot)\n",
    "gguf_path = \"/kaggle/input/modelsday4/model-q4_0.gguf\"\n",
    "\n",
    "# 2. Load Model\n",
    "# n_gpu_layers=-1 moves all layers to the GPU (T4 on Kaggle)\n",
    "llm = Llama(\n",
    "    model_path=gguf_path,\n",
    "    n_gpu_layers=-1, \n",
    "    n_ctx=2048,\n",
    "    verbose=False # Keeps the output clean\n",
    ")\n",
    "\n",
    "# 3. Define Prompt and Measure\n",
    "prompt = \"Explain the coronavirus\"\n",
    "formatted_prompt = f\"Q: {prompt} A:\"\n",
    "\n",
    "start_time = time.time()\n",
    "response = llm(\n",
    "    formatted_prompt,\n",
    "    max_tokens=100,\n",
    "    stop=[\"Q:\", \"\\n\"],\n",
    "    echo=False\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "# 4. Calculations\n",
    "latency = end_time - start_time\n",
    "tokens_generated = response[\"usage\"][\"completion_tokens\"]\n",
    "tps = tokens_generated / latency\n",
    "\n",
    "print(f\"--- GGUF Q4_0 Performance ---\")\n",
    "print(f\" Latency: {latency:.3f}s\")\n",
    "print(f\" Tokens/sec: {tps:.2f}\")\n",
    "print(f\" Output: {response['choices'][0]['text'].strip()}\")\n",
    "\n",
    "# Note: VRAM usage for GGUF is best checked via the 'nvidia-smi' command \n",
    "# in a separate cell while the model is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T20:22:56.688610Z",
     "iopub.status.busy": "2026-01-29T20:22:56.688288Z",
     "iopub.status.idle": "2026-01-29T20:23:03.832210Z",
     "shell.execute_reply": "2026-01-29T20:23:03.831439Z",
     "shell.execute_reply.started": "2026-01-29T20:22:56.688585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Setup Path (Based on your file browser screenshot)\n",
    "gguf_path = \"/kaggle/input/modelsday4/model-q4_0.gguf\"\n",
    "\n",
    "# 2. Load Model\n",
    "# n_gpu_layers=-1 moves all layers to the GPU (T4 on Kaggle)\n",
    "llm = Llama(\n",
    "    model_path=gguf_path,\n",
    "    n_gpu_layers=-1, \n",
    "    n_ctx=2048,\n",
    "    verbose=False # Keeps the output clean\n",
    ")\n",
    "\n",
    "# 3. Define Prompt and Measure\n",
    "prompt = \"Explain the coronavirus disease\"\n",
    "formatted_prompt = f\"Q: {prompt} A:\"\n",
    "\n",
    "start_time = time.time()\n",
    "response = llm(\n",
    "    formatted_prompt,\n",
    "    max_tokens=100,\n",
    "    stop=[\"Q:\", \"\\n\"],\n",
    "    echo=False\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "# 4. Calculations\n",
    "latency = end_time - start_time\n",
    "tokens_generated = response[\"usage\"][\"completion_tokens\"]\n",
    "tps = tokens_generated / latency\n",
    "\n",
    "print(f\"--- GGUF Q4_0 Performance ---\")\n",
    "print(f\"Latency: {latency:.3f}s\")\n",
    "print(f\" Tokens/sec: {tps:.2f}\")\n",
    "print(f\" Output: {response['choices'][0]['text'].strip()}\")\n",
    "\n",
    "# Note: VRAM usage for GGUF is best checked via the 'nvidia-smi' command \n",
    "# in a separate cell while the model is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9284324,
     "sourceId": 14536411,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9370595,
     "sourceId": 14667782,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
